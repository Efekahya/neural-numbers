{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn_som.som import SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataAndValuesFromXLSX (dataFile,resultFile):\n",
    "    # Read data from excel file\n",
    "    data = pd.read_excel(dataFile,index_col=0)\n",
    "    result = pd.read_excel(resultFile,index_col=0)\n",
    "\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add result column to data\n",
    "    concat = pd.concat([data,result[\"label\"]],axis=1)\n",
    "\n",
    "    return concat\n",
    "\n",
    "# print(\"Data and values from excel file: \", readDataAndValuesFromXLSX(\"dataset.xlsx\",\"index.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows (data):\n",
    "    # Drop result column\n",
    "    dataWithoutResult = data.drop([\"label\"],axis=1)\n",
    "\n",
    "    # Normalize data 0 to 1\n",
    "    dataWithoutResult = dataWithoutResult.apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=1)\n",
    "    # Add result column to data\n",
    "    concat = pd.concat([dataWithoutResult,data[\"label\"]],axis=1)\n",
    "\n",
    "    return concat\n",
    "# print(\"Normalize data: \", normalizeRows(readDataAndValuesFromXLSX(\"dataset.xlsx\",\"index.xlsx\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataTrainAndTest (data):\n",
    "    # Split data to train and test\n",
    "    train = data.sample(frac=0.5,random_state=42)\n",
    "    test = data.drop(train.index)\n",
    "\n",
    "    return train,test\n",
    "# print(splitDataTrainAndTest(readDataAndValuesFromXLSX(\"dataset.xlsx\",\"index.xlsx\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalizeRows(readDataAndValuesFromXLSX(\"dataset.xlsx\",\"index.xlsx\"))\n",
    "\n",
    "train,test = splitDataTrainAndTest(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainToNumPy = train.to_numpy()\n",
    "testToNumPy = test.to_numpy()\n",
    "\n",
    "iris_som = SOM(m=1,n=4,dim=784,random_state=12)\n",
    "iris_som.fit(trainToNumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data\n",
    "prediction = iris_som.predict(testToNumPy)\n",
    "\n",
    "# Save predictions to a file \n",
    "np.savetxt(\"kume-sonuc.txt\", np.dstack((np.arange(1, prediction.size+1),prediction))[0], fmt=\"%d\\t\\tC%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getEachAttributesAccuracy:  [0.3, 0.3090909090909091, 0.0, 0.0]\n",
      "Accuracy:  0.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def getEachAttributesAccuracy (trainToNumPy,predictions):\n",
    "    uniqueAttributes, counts = np.unique(trainToNumPy[:,-1], return_counts=True)\n",
    "    accuracyArray = []\n",
    "\n",
    "    for i in range(len(uniqueAttributes)):\n",
    "        correct = 0\n",
    "        for j in range(len(predictions)):\n",
    "            if trainToNumPy[j][-1] == uniqueAttributes[i] and predictions[j] == uniqueAttributes[i]:\n",
    "                correct += 1\n",
    "        accuracyArray.append(correct/counts[i])\n",
    "    return accuracyArray\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('getEachAttributesAccuracy: ', getEachAttributesAccuracy(trainToNumPy,prediction))\n",
    "correct=0\n",
    "convertPrediction = []\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] == 0:\n",
    "        convertPrediction.append(0)\n",
    "    elif prediction[i] == 1:\n",
    "        convertPrediction.append(3)\n",
    "    elif prediction[i] == 2:\n",
    "        convertPrediction.append(8)\n",
    "    elif prediction[i] == 3:\n",
    "        convertPrediction.append(9)\n",
    "\n",
    "\n",
    "for i in range(len(testToNumPy)):\n",
    "    if testToNumPy[i][-1] == convertPrediction[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(\"Accuracy: \", correct/len(testToNumPy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood function = Gaussian\n",
    "Initial learning rate = 1\n",
    "Learning rate decay = (1 - (global_iter_counter / total_iterations)) * self.initial_lr //linear decay\n",
    "Sigma decay = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
